{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "#set seed\n",
    "np.random.seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sampling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample the linear predictor, by first sampling the mean from a Redamacher distribution, and then sampling the predictor from a normal distribution around the mean\n",
    "# dim: the dimension of the data (+ 1 for the constant 1)\n",
    "# sigma: the standard deviation of the normal distribution\n",
    "# bias_factor: the scale of the Redamacher distribution\n",
    "def sample_predictor(dim, sigma, bias_factor):\n",
    "    mean = (np.random.binomial(1, 0.5, dim+1)*2 - 1)*bias_factor\n",
    "    return np.random.normal(mean, sigma)\n",
    "\n",
    "#Sample the data, by sampling from a normal distribution with mean zero and the given standard deviation, and add the constant 1\n",
    "# dim: the dimension of the data\n",
    "# data_scale: the standard deviation of the normal distribution\n",
    "# num_samples: the number of samples\n",
    "def sample_data(dim, data_scale, num_samples):\n",
    "    mean = np.zeros(dim)\n",
    "    data = np.random.normal(mean, data_scale, (num_samples, dim))\n",
    "    data = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "    return data\n",
    "\n",
    "#Sample the labels, by first computing the probability of the positive class according to the logistic distribution, and then sampling from a Bernoulli distribution with the corresponding probability\n",
    "# data: the data\n",
    "# predictor: the linear predictor of the logistic distribution\n",
    "def sample_labels(data, predictor):\n",
    "    prob = 1/(1+np.exp(-np.dot(data, predictor)))\n",
    "    labels = np.random.binomial(1, prob)\n",
    "    return labels\n",
    "\n",
    "#Sample the data, a predictor, and the labels using the above functions\n",
    "def sample_data_and_labels(data_parameters):\n",
    "    predictor = sample_predictor(data_parameters['dim'], data_parameters['init_sigma'], data_parameters['bias_factor'])\n",
    "    \n",
    "    training_data = sample_data(data_parameters['dim'], data_parameters['data_scale'], data_parameters['training_sample_size'])\n",
    "    training_labels = sample_labels(training_data, predictor)\n",
    "    training_set = {'data': training_data, 'labels': training_labels}\n",
    "\n",
    "    test_sample_size = int(data_parameters['training_sample_size'] * data_parameters['test_ratio'])\n",
    "    test_data = sample_data(data_parameters['dim'], data_parameters['data_scale'], test_sample_size)\n",
    "    test_labels = sample_labels(test_data, predictor)\n",
    "    test_set = {'data': test_data, 'labels': test_labels}\n",
    "    \n",
    "    full_data = {'training': training_set, 'test': test_set}\n",
    "    return full_data, predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the logistic loss of the given predictor on the given data set\n",
    "# data_set: a dictionary containing the data and labels\n",
    "# predictor: the linear predictor of the logistic distribution\n",
    "\n",
    "def logistic_loss(data_set, predictor):\n",
    "    #clip the probability to avoid numerical instability\n",
    "    prob = np.clip(1/(1+np.exp(-np.dot(data_set['data'], predictor))), 1e-4, 1-1e-4)\n",
    "    return -np.mean(data_set['labels'] * np.log(prob) + (1-data_set['labels']) * np.log(1-prob))\n",
    "\n",
    "#Compute the logistic loss of the given predictor on the given data set using gradient descent\n",
    "# starting_predictor: the initial predictor\n",
    "# data_set: a dictionary containing the data and labels\n",
    "# learning_parameters: a dictionary containing the learning parameters\n",
    "def logistic_regression(starting_predictor, data_set, learning_parameters):\n",
    "    predictor = starting_predictor.copy()\n",
    "    for i in range(learning_parameters['num_of_iterations']):\n",
    "        prob = np.clip(1/(1+np.exp(-np.dot(data_set['data'], predictor))), 1e-4, 1-1e-4)\n",
    "        full_gradient = data_set['data'] * (data_set['labels'] - prob)[:, np.newaxis]\n",
    "        gradient = np.mean(full_gradient, axis=0)\n",
    "        predictor += learning_parameters['learning_rate'] * gradient\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to store the losses of different predictors\n",
    "class Losses:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.loss = {}\n",
    "    \n",
    "    def __init__(self, name, data_set = [], predictors = []):\n",
    "        self.name = name\n",
    "        self.loss = {}\n",
    "        for key in predictors:\n",
    "            self.loss[key + ' predictor'] = logistic_loss(data_set, predictors[key])\n",
    "\n",
    "    def add_loss(self, other):\n",
    "        for key in other.loss:\n",
    "            if key in self.loss:\n",
    "                self.loss[key] += other.loss[key]\n",
    "            else:\n",
    "                self.loss[key] = other.loss[key]\n",
    "\n",
    "    def average_loss(self, num_of_experiments):\n",
    "        for key in self.loss:\n",
    "            self.loss[key] /= num_of_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single experiment, which samples the data and the predictor, and then computes the losses of the true predictor, the guess predictor, and the learned predictors, one per learning_parameters\n",
    "# data_parameters: a dictionary containing the data sampling parameters\n",
    "# learning_parameters_array: an array of dictionaries, each containing a set of learning parameters\n",
    "def run_experiment(data_parameters, learning_parameters_array):\n",
    "    full_data, true_predictor = sample_data_and_labels(data_parameters)\n",
    "    guess_predictor = sample_predictor(data_parameters['dim'], data_parameters['init_sigma'], data_parameters['bias_factor'])\n",
    "    predictors = {'true': true_predictor, 'guess': guess_predictor}\n",
    "    for i in range(len(learning_parameters_array)):\n",
    "        predictors[(learning_parameters_array[i])['name']] = logistic_regression(guess_predictor, full_data['training'], learning_parameters_array[i])\n",
    "\n",
    "    training_losses = Losses('training', full_data['training'], predictors)\n",
    "    test_losses = Losses('test', full_data['test'], predictors)\n",
    "    return training_losses, test_losses\n",
    "\n",
    "# A full experiment, which runs num_of_experiments experiments using the previous function and averages the losses\n",
    "# data_parameters: a dictionary containing the data sampling parameters\n",
    "# learning_parameters_array: an array of dictionaries, each containing a set of learning parameters\n",
    "# num_of_experiments: the number of experiments\n",
    "def run_full_experiment(data_parameters, learning_parameters_array, num_of_experiments):\n",
    "    training_losses = Losses('training')\n",
    "    test_losses = Losses('test')\n",
    "\n",
    "    for i in range(num_of_experiments):\n",
    "        iter_training_losses, iter_test_losses = run_experiment(data_parameters, learning_parameters_array)\n",
    "        training_losses.add_loss(iter_training_losses)\n",
    "        test_losses.add_loss(iter_test_losses)\n",
    "\n",
    "    training_losses.average_loss(num_of_experiments)\n",
    "    test_losses.average_loss(num_of_experiments)            \n",
    "    return training_losses, test_losses\n",
    "\n",
    "def float2txt(num):\n",
    "    return \"{:.3f}\".format(num)\n",
    "\n",
    "#Print the results of the experiment\n",
    "# data_parameters: a dictionary containing the data sampling parameters\n",
    "# training_losses: the losses on the training set\n",
    "# test_losses: the losses on the test set\n",
    "def print_results(data_parameters, training_losses, test_losses):\n",
    "    print('Losses for dim = ', str(data_parameters['dim']), ', data_scale = ', str(data_parameters['data_scale']), ', training_sample_size = ', str(data_parameters['training_sample_size']))\n",
    "    \n",
    "    table = PrettyTable(['predictor', 'Training', 'test'])\n",
    "    for key in training_losses.loss:\n",
    "        table.add_row([key, float2txt(training_losses.loss[key]), float2txt(test_losses.loss[key])])\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses for dim =  10 , data_scale =  2 , training_sample_size =  300\n",
      "+-------------------+----------+-------+\n",
      "|     predictor     | Training |  test |\n",
      "+-------------------+----------+-------+\n",
      "|   true predictor  |  0.180   | 0.179 |\n",
      "|  guess predictor  |  2.569   | 2.568 |\n",
      "| Learned predictor |  0.170   | 0.199 |\n",
      "+-------------------+----------+-------+\n"
     ]
    }
   ],
   "source": [
    "data_parameters = {'dim': 10, 'data_scale': 2, 'init_sigma': 0.5, 'bias_factor': 1, 'training_sample_size': 300, 'test_ratio': 0.5}\n",
    "learning_parameters = [{'name': 'Learned', 'learning_rate': 1, 'num_of_iterations': 40}]\n",
    "num_of_experiments   = 1000\n",
    "\n",
    "training_losses, test_losses = run_full_experiment(data_parameters, learning_parameters, num_of_experiments)\n",
    "print_results(data_parameters, training_losses, test_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerasenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
